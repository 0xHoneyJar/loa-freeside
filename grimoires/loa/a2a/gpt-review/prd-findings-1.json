{
  "verdict": "CHANGES_REQUIRED",
  "summary": "As written, the PRD risks breaking core invariants and tenant isolation via governance-driven limit changes and an underspecified event-sequencing model that can’t safely support replay/consistency under concurrency.",
  "blocking_issues": [
    {
      "location": "F-4 Governance Layer — Integration with Conservation Guard (budget_limit policies feed into I-1 limit; policy transitions atomic)",
      "issue": "Governance can change the budget limit without specifying safety constraints relative to existing committed/reserved amounts, which can immediately violate the conservation/budget safety invariant (committed + reserved + available = limit) or force available negative.",
      "why_blocking": "If a community approves a lower limit than current committed+reserved, the system either (a) violates invariants, (b) retroactively invalidates already-authorized reservations/commits, or (c) deadlocks/DoSes all future operations. Any of these breaks the core promise of cycle-037 and can cause production incidents or incorrect balances.",
      "fix": "Define hard rules for limit decreases: e.g., enforce `new_limit >= committed + reserved` at approval/enforcement time (or schedule `effective_from` in the future and only activate once usage drops below). Specify what happens if a policy is approved but not yet enforceable (pending state). Add acceptance criteria + tests for limit increase/decrease, including concurrent reservations during policy change."
    },
    {
      "location": "F-3 Event Sourcing Formalization — Per-community monotonic sequence (CREATE SEQUENCE lot_entries_seq_per_community; or application-level table)",
      "issue": "The PRD proposes a single PostgreSQL SEQUENCE for “per-community monotonic sequence,” which does not produce per-community monotonicity and will not support deterministic replay ordering per tenant.",
      "why_blocking": "Replay correctness and consistency verification depend on a total order per community. A global sequence interleaves communities; an application-level approach without a precise locking/transaction pattern can produce gaps, duplicates, or out-of-order assignment under concurrency—making replay nondeterministic and breaking AC-3.2/AC-3.5 in real load.",
      "fix": "Pick one concrete, safe sequencing design: (1) `community_sequences` table with `SELECT ... FOR UPDATE` to allocate next sequence inside the same transaction as inserting lot_entries; or (2) advisory lock keyed by community_id + a per-community counter row. Document the exact transaction boundaries and failure semantics. Add AC that sequence_number is strictly increasing per community even under concurrent debits/reserves."
    },
    {
      "location": "F-3 Event Sourcing — Event Types vs existing double-entry ledger semantics",
      "issue": "Event types conflate “double-entry append-only ledger” with “single event per action” without specifying how double-entry is represented in the canonical log (one row per side vs one envelope with two postings).",
      "why_blocking": "If the canonical log is ambiguous, replayState can be implemented in incompatible ways (e.g., summing debits/credits vs reconstructing postings), leading to mismatches with the existing lot-based materialized state and potentially weakening invariants. This is a classic way to build the wrong thing while still “passing” superficial tests.",
      "fix": "Explicitly define the canonical event model: either (A) lot_entries remain the posting-level log (each row is a posting; correlation_id groups postings), or (B) introduce a separate `economic_events` envelope table and keep lot_entries as derived postings. Then define replay rules precisely (how reserve/release/debit map to postings, how multi-lot splits are represented, and what constitutes the source of truth). Update AC-3.2 to validate against posting-level totals and lot-level allocation."
    },
    {
      "location": "NF-3 Security + F-4 Governance API (membership verification; no privilege escalation; RLS on economic_policies only)",
      "issue": "Tenant isolation/security is only specified for the new economic_policies table, but governance changes affect enforcement in the conservation guard and potentially other tenant-scoped tables; the PRD doesn’t specify how requests are authorized end-to-end (who can propose/approve, how roles are represented, how RLS interacts with service roles).",
      "why_blocking": "Governance is a privileged control plane. If authorization is underspecified, it’s easy to accidentally allow cross-community policy writes/reads (via service role bypass) or allow any community member/agent to change budget limits. That’s a direct economic manipulation vector and can cause catastrophic loss of trust.",
      "fix": "Define an explicit authorization model: roles (member/operator/admin), who can propose vs approve, and how Arrakis token-gate maps to roles. Specify whether the API uses a Postgres service role (bypassing RLS) and, if so, require explicit `community_id` scoping checks plus audit logging. Add ACs for negative tests: cross-tenant access denied; non-approver cannot approve; agent cannot escalate privileges."
    },
    {
      "location": "F-2 Economic Velocity — Core computation uses linear regression + acceleration with BigInt fields; estimated_exhaustion_hours is number",
      "issue": "The algorithm as described requires fractional arithmetic (regression slopes/second derivatives) but the PRD mandates BigInt-only monetary arithmetic; it doesn’t specify a fixed-point strategy, rounding rules, or how to avoid float usage in the economic path.",
      "why_blocking": "Teams commonly “just use number” for regression, which violates the BigInt-only requirement and can introduce subtle drift/incorrect alerts (and potentially incorrect policy enforcement if alerts feed governance/auto-replenish later). This is a predictable implementation trap that can break a critical platform constraint.",
      "fix": "Specify fixed-point math: e.g., compute velocity in `micro_per_hour` using integer bucket sums and integer division with defined rounding (floor/nearest) and carry a `velocity_numer/velocity_denom` rational or scaled BigInt (e.g., per-second in micros, or micros * 1e6). Keep all monetary quantities BigInt; only convert to number for display fields outside enforcement. Add AC that no floating-point is used in velocity computation code paths (lint/test)."
    },
    {
      "location": "F-1 Purpose Field — Schema change uses TEXT default; PRD calls it an enum; success metric references usage_events",
      "issue": "Purpose is defined as an enum but implemented as free-form TEXT, and the success metric says “100% of usage_events have purpose populated” while the feature is specified as “purpose on lot_entries”; usage_events schema/relationship is not defined here.",
      "why_blocking": "This can lead to building inconsistent attribution (usage_events vs lot_entries) and makes it impossible to guarantee the stated metric. Free-form TEXT also allows unbounded cardinality (typos/new values) which breaks aggregation, governance “purpose_allocation,” and any per-purpose velocity logic.",
      "fix": "Choose the source of truth for purpose: either (1) purpose lives on lot_entries only and usage_events derives it, or (2) purpose is first-class on usage_events and copied into lot_entries at debit time. Enforce a constrained set: Postgres ENUM or a reference table with FK + migration path. Update the metric to match the chosen truth source and add ACs for validation (reject unknown purpose unless explicitly allowed)."
    },
    {
      "location": "NF-2 Performance targets + F-3 replay (≤500ms for 10k events) + RLS on tenant tables",
      "issue": "Replay and velocity queries are specified without addressing RLS overhead and indexing strategy for time-windowed and sequence scans; additionally, the PRD introduces a partial index on (community_id, sequence_number) but doesn’t guarantee sequence_number is present for all new rows or define backfill strategy for active communities.",
      "why_blocking": "If sequence_number is nullable for a long time or inconsistently populated, replay queries either miss events or require slow fallback ordering by created_at/event_id, which can violate deterministic ordering and blow the 500ms target. This can make F-3 unusable and block F-4 auditability.",
      "fix": "Require sequence_number to be NOT NULL for all new entries once FEATURE_EVENT_SOURCING is enabled for a community (community-level rollout). Define ordering fallback for legacy rows (e.g., treat legacy as sequence 0..N assigned by a one-time backfill job per community). Specify indexes for (community_id, created_at) for velocity windows and (community_id, sequence_number) for replay, and confirm RLS-safe query plans with EXPLAIN in ACs."
    }
  ],
  "question": "",
  "iteration": 1
}
